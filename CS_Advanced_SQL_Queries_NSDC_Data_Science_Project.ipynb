{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<h1 align=\"center\">\n",
        "    NSDC Data Science Projects\n",
        "</h1>\n",
        "\n",
        "<h2 align=\"center\">\n",
        "    Project: Advanced SQL Queries - #3\n",
        "</h2>\n",
        "\n",
        "<h3 align=\"center\">\n",
        "    Name: Sohini Chintala\n",
        "</h3>\n"
      ],
      "metadata": {
        "id": "tt70feOIfcQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Description:**\n",
        "\n",
        "To work with the New York City Airbnb dataset in this project, you'll need to create and connect to an SQLite database as seen in the steps within this notebook.\n",
        "\n",
        "**Key Features**\n",
        "\n",
        "- Practical SQL exercises with step-by-step instructions.\n",
        "- Real-world datasets for hands-on experience.\n",
        "- Comprehensive explanations of SQL queries and concepts.\n",
        "- Interactive coding examples and challenges.\n",
        "\n",
        "**Dataset**\n",
        "- You can find the New York City Airbnb dataset on Kaggle: [New York City Airbnb Dataset](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data)\n",
        "- Download the dataset from the provided link and save it as 'AB_NYC_2019.csv' on your computer.\n",
        "- On colab, click on the \"Upload\" button and select the 'AB_NYC_2019.csv' file from your local computer."
      ],
      "metadata": {
        "id": "35fgH_hXgAUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "This SQL project is designed for intermediate to advanced learners who want to practice writing SQL queries using a real-world dataset. In this project, we will work with the New York City Airbnb dataset. The main goal is to practice SQL syntax and advance data analysis tasks. Make sure to complete the code where there is a 'TODO' sign. Hints are provided along the way.\n",
        "\n",
        "## Table of Contents\n",
        "1. [Libraries and Database Setup](#database-setup)\n",
        "2. [Loading the Dataset](#loading-dataset)\n",
        "3. [Advanced SQL Queries](#sql-queries)\n",
        "4. [Closing Connection](#closing-connection)\n",
        "5. [Summary of Advanced SQL Commands](#summary-commands)\n",
        "\n",
        "# Section 1: Libraries and Database Setup <a name=\"database-setup\"></a>\n",
        "\n",
        "### Task 1.1: Import Libraries\n",
        "Let's start by importing the necessary libraries.\n"
      ],
      "metadata": {
        "id": "I4xHzjkqigQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google Drive\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuwWeEnIiirc",
        "outputId": "54ea6134-e6dc-47a3-e8c0-fe8fa2ac8e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3"
      ],
      "metadata": {
        "id": "NnR6oXHNG6Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.2: Connect to the Database\n",
        "\n",
        "Next, establish a connection to the SQLite database named 'airbnb.db'.\n",
        "Use the sqlite3 library to create a connection object and store it in a variable called 'conn'.\n"
      ],
      "metadata": {
        "id": "qSERoLykixcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Completed\n",
        "\n",
        "# Connect to the SQLite database\n",
        "conn = sqlite3.connect('airbnb.db')\n",
        "cursor = conn.cursor()"
      ],
      "metadata": {
        "id": "8G9tDLd2iyq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: Loading the Dataset <a name=\"loading-dataset\"></a>"
      ],
      "metadata": {
        "id": "gCE_OEk5BrF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.1: Load the Dataset\n",
        "- You will work with the 'listings' table from the New York City Airbnb dataset.\n",
        "- Load the dataset from the CSV file 'AB_NYC_2019.csv' into a DataFrame named 'df'.\n",
        "- Then, import the DataFrame into the 'listings' table in the database using the to_sql() method.\n",
        "\n",
        "\\\\\n",
        "\n",
        "Hint: Use the read_csv() method from pandas to read the CSV file.\n"
      ],
      "metadata": {
        "id": "_bXjXLHli0aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Completed\n",
        "\n",
        "# Load the dataset into the SQLite database\n",
        "data_path = 'AB_NYC_2019.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "df.to_sql('listings', conn, if_exists='replace', index=False)"
      ],
      "metadata": {
        "id": "blwNI_PEi5FT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "3a35a875-fb03-461e-e11a-4713156e0b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'AB_NYC_2019.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-7d30519c48b3>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the dataset into the SQLite database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'AB_NYC_2019.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'listings'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AB_NYC_2019.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: Advanced SQL Tasks <a name=\"sql-queries\"></a>\n",
        "\n",
        "Query the database: Write SQL queries to analyze the data in the SQLite database. You can use the sqlite3 library to execute the queries and fetch the results."
      ],
      "metadata": {
        "id": "1j3xgaK4Ck3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.1: Analyzing Host Performance\n",
        "- Find the top 10 hosts with the highest average ratings.\n",
        "- Include host's name, number of reviews, and average rating."
      ],
      "metadata": {
        "id": "eKaLdB8Fi7Bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here:\n",
        "\n",
        "query = \"SELECT host_id, host_name, SUM(number_of_reviews) as num_reviews, AVG(reviews_per_month) as avg_rating from listings GROUP BY host_name, host_id ORDER BY avg_rating DESC LIMIT 10\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "-- 1. Calculate the number of reviews received by each host and name the result as 'num_reviews'\n",
        "-- 2. Calculate the average rating (reviews per month) for each host and name the result as 'avg_rating'\n",
        "-- 3. Group the results by host name\n",
        "-- 4. Order the results by average rating in descending order\n",
        "-- 5. Display the top 10 hosts with their names, the number of reviews, and average rating\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gqe3KmtUCr9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Completed\n",
        "\n",
        "result = pd.read_sql_query(query, conn)\n",
        "print(\"\\nAnalyzing Host Performance:\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "6fib2hIqjKWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.2: Analyzing Monthly Price Trends\n",
        "- Display the month, average price, and number of listings for each month."
      ],
      "metadata": {
        "id": "0O4Nfb9yC61D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here:\n",
        "\n",
        "query = \"SELECT strftime('%m', last_review) AS month, AVG(price) as avg_price, SUM(calculated_host_listings_count) as num_listings from listings GROUP BY month ORDER BY avg_price desc\"\n",
        "\n",
        "\"\"\"\n",
        "-- 1. Use the `strftime('%m', last_review)` function to extract the month from the 'last_review' column and name it as 'month'\n",
        "-- 2. Calculate the average price for listings in each month and name the result as 'avg_price'\n",
        "-- 3. Calculate the number of listings in each month and name the result as 'num_listings'\n",
        "-- 4. Group the results by the 'month' column\n",
        "-- 5. Order the results by average price in descending order\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4fJa4Bt6C7Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Completed\n",
        "\n",
        "result = pd.read_sql_query(query, conn)\n",
        "print(\"Analyzing Monthly Price Trends:\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "_KICYQII-AVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.3: Find the top 5 neighborhoods with the highest price variability\n",
        "-  Find the top 5 neighborhoods with the highest price variability.\n",
        "-  Display neighborhood and price variability.\n"
      ],
      "metadata": {
        "id": "uULLKoO0DEou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here:\n",
        "\n",
        "query = \"WITH price_variability AS (SELECT neighbourhood, AVG(price) as avg_price, MAX(price) as max_price, min(price) as min_price from listings group by neighbourhood) SELECT neighbourhood, (max_price - min_price) as pv from price_variability ORDER BY pv desc limit 5\"\n",
        "\n",
        "\"\"\"\n",
        "-- 1. Create a Common Table Expression (CTE) named 'price_variability' that calculates the following for each neighborhood:\n",
        "--    - Calculate the average price and name it as 'avg_price'\n",
        "--    - Calculate the maximum price and name it as 'max_price'\n",
        "--    - Calculate the minimum price and name it as 'min_price'\n",
        "-- 2. In the main query, select the 'neighbourhood' column and calculate the price variability as the difference between 'max_price' and 'min_price'\n",
        "-- 3. Order the results by price variability in descending order\n",
        "-- 4. Limit the results to the top 5 neighborhoods\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EDnyQuz1DDin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Completed\n",
        "\n",
        "result = pd.read_sql_query(query, conn)\n",
        "print(\"Advanced Aggregation:\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "ZGIUx6pn1ZpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.4: Advanced Data Manipulation\n",
        "- Calculate days since the last review and identify top 10 listings.\n",
        "- Display listing ID, name, and days since the last review."
      ],
      "metadata": {
        "id": "yPvuIfq7DMhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here:\n",
        "\n",
        "query = \"SELECT id, name, julianday('now') - julianday(last_review) as days_since_last_review from listings Where last_review is not null order by days_since_last_review desc limit 10\"\n",
        "\n",
        "\"\"\"\n",
        "-- 1. Select the following columns from the 'listings' table:\n",
        "--    - 'id'\n",
        "--    - 'name'\n",
        "--    - Calculate the number of days since the last review and name it as 'days_since_last_review.'\n",
        "--      Use the 'julianday' function to calculate the difference in days between the current date and the 'last_review' date\n",
        "-- 2. Order the results by 'days_since_last_review' in descending order\n",
        "-- 3. Limit the results to the top 10 listings\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "U1r32q1rDOil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Completed\n",
        "\n",
        "result = pd.read_sql_query(query, conn)\n",
        "print(\"\\nAdvanced Data Manipulation:\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "s93h67wkDR6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.5:  Identifying Top Hosts by Revenue\n",
        "- Calculate the total revenue generated by each host and rank them based on revenue.\n",
        "- Display the host name, total revenue, and their rank."
      ],
      "metadata": {
        "id": "nIWy051Y2j7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here:\n",
        "\n",
        "query = \"WITH new_table as (SELECT host_name, price*minimum_nights as total_revenue from listings) SELECT host_name, total_revenue, RANK() over (ORDER BY total_revenue desc) as revenue_rank from new_table group by host_name order by total_revenue desc limit 10\"\n",
        "\n",
        "\"\"\"\n",
        "-- 1. Select the following columns from the 'listings' table:\n",
        "--    - 'host_name'\n",
        "--    - Calculate the total revenue for each host by multiplying 'price' by 'minimum_nights' and name it as 'total_revenue.'\n",
        "--    - Use the RANK() window function to determine the rank of each host by total revenue in descending order and name it as 'revenue_rank.'\n",
        "-- 2. Group the results by 'host_name.'\n",
        "-- 3. Order the results by 'total_revenue' in descending order.\n",
        "-- 4. Limit the results to the top 10 hosts.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gygsx68g2YEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Completed\n",
        "\n",
        "result = pd.read_sql_query(query, conn)\n",
        "print(\"Identifying Top Hosts by Revenue:\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "BOZr1Ty02YHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Closing the Database Connection <a name=\"closing-connection\"></a>"
      ],
      "metadata": {
        "id": "CyaxVtAuDyDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4.1: Close the Cursor and Database Connection\n",
        "- It's good practice to close the cursor and the database connection when you're done working with the database to free up system resources and maintain proper connection management.\n"
      ],
      "metadata": {
        "id": "nFKyd0DbD4Ns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Completed\n",
        "\n",
        "# Make sure to include these lines of code at the end of your script to properly close the cursor and database connection.\n",
        "cursor.close()\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "_z_H6ZsmD2GX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5: Summary of Advanced SQL Commands <a name=\"summary-commands\"></a>"
      ],
      "metadata": {
        "id": "5tOYpl9rVXV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, you have practiced various SQL commands and tasks. Below is a summary of the advanced SQL commands used:\n",
        "\n",
        "**Task 1.2: Connect to the Database**\n",
        "- Established a connection to the SQLite database.\n",
        "\n",
        "**Task 2.1: Load the Dataset**\n",
        "- Loaded the dataset into a DataFrame.\n",
        "- Imported the DataFrame into the database.\n",
        "\n",
        "**Task 3.1: Identifying Top Hosts by Reviews and Ratings**\n",
        "- SQL Commands: SELECT, COUNT(), AVG(), GROUP BY, ORDER BY, LIMIT\n",
        "- Description: Calculate the number of reviews and average ratings for each host and identify the top hosts by ratings and reviews.\n",
        "\n",
        "**Task 3.2: Analyzing Seasonal Price Trends**\n",
        "- SQL Commands: SELECT, strftime(), AVG(), COUNT(), GROUP BY, ORDER BY\n",
        "- Description: Analyze seasonal price trends by extracting the month from the last review date and calculating average prices.\n",
        "\n",
        "**Task 3.3: Finding Neighborhoods with Price Variability**\n",
        "- SQL Commands: WITH, SELECT, AVG(), MAX(), MIN(), GROUP BY, ORDER BY\n",
        "- Description: Identify neighborhoods with the highest price variability by calculating the difference between the maximum and minimum prices.\n",
        "\n",
        "**Task 3.4: Advanced Data Manipulation**\n",
        "- SQL Commands: SELECT, julianday(), ORDER BY, LIMIT\n",
        "- Description: Calculate the number of days since the last review for each listing and sort them by the days since the last review.\n",
        "\n",
        "**Task 3.5: Identifying Top Hosts by Revenue**\n",
        "- SQL Commands: SELECT, SUM(), RANK(), GROUP BY, ORDER BY, LIMIT\n",
        "- Description: Determine the top hosts by total revenue generated from their listings, including a ranking based on revenue.\n",
        "\n",
        "**Task 4.1: Close the Cursor and Database Connection**\n",
        "- Closed the cursor and the database connection.\n",
        "\n",
        "This summary provides an overview of the advanced SQL commands used in this project. You can use this as a reference for future SQL projects and data analysis tasks."
      ],
      "metadata": {
        "id": "wS_xxvBKVTha"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT8Rpdt4ZPeA"
      },
      "source": [
        "<h3 align = 'center' >\n",
        "Thank you for completing the project!\n",
        "</h3>\n",
        "\n",
        "Please share your completed Google Colab Notebook with nsdc@nebigdatahub.org to receive a certificate of completion. Do reach out to us if you have any questions or concerns. We are here to help you learn and grow.\n",
        "\n",
        "If you have any queries, please contact the NSDC HQ Team at nsdc@nebigdatahub.org.\n"
      ]
    }
  ]
}